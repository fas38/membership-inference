{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NUkPg0Dx-_ew"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "import pickle\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import resnet34, mobilenet_v2\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "import random\n",
        "\n",
        "# set random seed\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye7G-npX__yB",
        "outputId": "219a8648-d8d4-4fa7-fc21-5fa6f78dfa7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46KTyQoyAf-H",
        "outputId": "496caee9-85da-43a6-9de3-b9d17dbfb6c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "<class 'list'> 50000\n"
          ]
        }
      ],
      "source": [
        "DATA_PATH = '/content/drive/My Drive/Colab Notebooks/attack/pickle/tinyimagenet/mobilenetv2/shadow.p'\n",
        "\n",
        "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "with open(DATA_PATH, \"rb\") as f:\n",
        "    dataset = pickle.load(f)\n",
        "\n",
        "print(type(dataset), len(dataset))\n",
        "\n",
        "# split the dataset\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMoRjJvFBKUQ"
      },
      "source": [
        "# shadow model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lFOcEBMABRLL",
        "outputId": "0ccc7f60-7420-4239-d7b2-9bde9251548a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 81/200, Loss: 1.049952148604393\n",
            "Epoch 82/200, Loss: 0.9497194398403168\n",
            "Epoch 83/200, Loss: 0.9521079958438873\n",
            "Epoch 84/200, Loss: 0.9292616271972656\n",
            "Epoch 85/200, Loss: 0.9254152151107788\n",
            "Epoch 86/200, Loss: 0.8778655840396881\n",
            "Epoch 87/200, Loss: 0.8680716379165649\n",
            "Epoch 88/200, Loss: 0.8405036175251007\n",
            "Epoch 89/200, Loss: 0.8080744936466216\n",
            "Epoch 90/200, Loss: 0.8046137293815613\n",
            "Epoch 91/200, Loss: 0.4122590695142746\n",
            "Epoch 92/200, Loss: 0.2570977275967598\n",
            "Epoch 93/200, Loss: 0.19119677765369417\n",
            "Epoch 94/200, Loss: 0.15551899327635765\n",
            "Epoch 95/200, Loss: 0.1298128958582878\n",
            "Epoch 96/200, Loss: 0.11229989638924599\n",
            "Epoch 97/200, Loss: 0.09779601016044617\n",
            "Epoch 98/200, Loss: 0.08546636273860932\n",
            "Epoch 99/200, Loss: 0.07854265461564064\n",
            "Epoch 100/200, Loss: 0.07072341081798077\n",
            "Epoch 101/200, Loss: 0.061102390989661214\n",
            "Epoch 102/200, Loss: 0.05657731387317181\n",
            "Epoch 103/200, Loss: 0.0537051407366991\n",
            "Epoch 104/200, Loss: 0.05217891236245632\n",
            "Epoch 105/200, Loss: 0.050566217915713786\n",
            "Epoch 106/200, Loss: 0.0501400958314538\n",
            "Epoch 107/200, Loss: 0.049043722143769265\n",
            "Epoch 108/200, Loss: 0.04737208697199821\n",
            "Epoch 109/200, Loss: 0.04644878140538931\n",
            "Epoch 110/200, Loss: 0.045041777482628824\n",
            "Epoch 111/200, Loss: 0.04372369410693645\n",
            "Epoch 112/200, Loss: 0.045675145940482616\n",
            "Epoch 113/200, Loss: 0.046911637091636656\n",
            "Epoch 114/200, Loss: 0.04502222685366869\n",
            "Epoch 115/200, Loss: 0.04402935929447412\n",
            "Epoch 116/200, Loss: 0.04688061442524195\n",
            "Epoch 117/200, Loss: 0.04328654742240906\n",
            "Epoch 118/200, Loss: 0.04375769048780203\n",
            "Epoch 119/200, Loss: 0.045059409445524216\n",
            "Epoch 120/200, Loss: 0.044127034316957\n",
            "Epoch 121/200, Loss: 0.044026433782279495\n",
            "Epoch 122/200, Loss: 0.04448045592457056\n",
            "Epoch 123/200, Loss: 0.04403939406871796\n",
            "Epoch 124/200, Loss: 0.04253803897053003\n",
            "Epoch 125/200, Loss: 0.04401810619086027\n",
            "Epoch 126/200, Loss: 0.045119506070017815\n",
            "Epoch 127/200, Loss: 0.043617169201374054\n",
            "Epoch 128/200, Loss: 0.04433383883535862\n",
            "Epoch 129/200, Loss: 0.04376841481328011\n",
            "Epoch 130/200, Loss: 0.044792812310159205\n",
            "Epoch 131/200, Loss: 0.044859390437602994\n",
            "Epoch 132/200, Loss: 0.0440261617988348\n",
            "Epoch 133/200, Loss: 0.04456190251559019\n",
            "Epoch 134/200, Loss: 0.042521796143054964\n",
            "Epoch 135/200, Loss: 0.043654497998952865\n",
            "Epoch 136/200, Loss: 0.04443278272300959\n",
            "Epoch 137/200, Loss: 0.04364836517125368\n",
            "Epoch 138/200, Loss: 0.0440146797567606\n",
            "Epoch 139/200, Loss: 0.045245712031424044\n",
            "Epoch 140/200, Loss: 0.044194608779251576\n",
            "Epoch 141/200, Loss: 0.044635587683320044\n",
            "Epoch 142/200, Loss: 0.04464877034574747\n",
            "Epoch 143/200, Loss: 0.0441587559774518\n",
            "Epoch 144/200, Loss: 0.0429449421107769\n",
            "Epoch 145/200, Loss: 0.04416853684186935\n",
            "Epoch 146/200, Loss: 0.043428326058387756\n",
            "Epoch 147/200, Loss: 0.0441995497956872\n",
            "Epoch 148/200, Loss: 0.04460511351376772\n",
            "Epoch 149/200, Loss: 0.045505248127877714\n",
            "Epoch 150/200, Loss: 0.044451510612666605\n",
            "Epoch 151/200, Loss: 0.04480588684082031\n",
            "Epoch 152/200, Loss: 0.04514986394792795\n",
            "Epoch 153/200, Loss: 0.04426448136866093\n",
            "Epoch 154/200, Loss: 0.0445061264783144\n",
            "Epoch 155/200, Loss: 0.04466969216763973\n",
            "Epoch 156/200, Loss: 0.045280684465169904\n",
            "Epoch 157/200, Loss: 0.045107985106110575\n",
            "Epoch 158/200, Loss: 0.044751808752119544\n",
            "Epoch 159/200, Loss: 0.04371953711360693\n",
            "Epoch 160/200, Loss: 0.04262641199529171\n",
            "Epoch 161/200, Loss: 0.04335648250430822\n",
            "Epoch 162/200, Loss: 0.04284418550431728\n",
            "Epoch 163/200, Loss: 0.04302083022296429\n",
            "Epoch 164/200, Loss: 0.04367868149876594\n",
            "Epoch 165/200, Loss: 0.04346744292229414\n",
            "Epoch 166/200, Loss: 0.043231170988082886\n",
            "Epoch 167/200, Loss: 0.04454845923930407\n",
            "Epoch 168/200, Loss: 0.04550644186735153\n",
            "Epoch 169/200, Loss: 0.042830465571582314\n",
            "Epoch 170/200, Loss: 0.04337195591777563\n",
            "Epoch 171/200, Loss: 0.043834352830052374\n",
            "Epoch 172/200, Loss: 0.043870622761547566\n",
            "Epoch 173/200, Loss: 0.044643987664580345\n",
            "Epoch 174/200, Loss: 0.04367944141030312\n",
            "Epoch 175/200, Loss: 0.04417597377151251\n",
            "Epoch 176/200, Loss: 0.04453095044642687\n",
            "Epoch 177/200, Loss: 0.04475459821969271\n",
            "Epoch 178/200, Loss: 0.04390554223656654\n",
            "Epoch 179/200, Loss: 0.04473900197893381\n",
            "Epoch 180/200, Loss: 0.04466577300131321\n",
            "Epoch 181/200, Loss: 0.044436404357850554\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-159-ed0ce3a1f54e>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshadow_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/mobilenetv2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/mobilenetv2.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;31m# This exists since TorchScript doesn't support inheritance, so the superclass method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m# (this one) needs to have a name other than `forward` that can be accessed in a subclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;31m# Cannot use \"squeeze\" as batch-size can be 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madaptive_avg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/mobilenetv2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_res_connect\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \"\"\"\n\u001b[0;32m--> 175\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2507\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2509\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2510\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2511\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# model parameters\n",
        "n_classes = 200\n",
        "n_epochs = 200\n",
        "batch_size = 256\n",
        "learning_rate = 0.001\n",
        "weight_decay = 1e-4  # parameter for L2 regularization\n",
        "\n",
        "\n",
        "# load target model for shadow model\n",
        "shadow_model = mobilenet_v2(num_classes=n_classes).to(device)\n",
        "state_dict = torch.load('/content/drive/My Drive/Colab Notebooks/attack/models/mobilenetv2_tinyimagenet.pth', map_location=device)\n",
        "shadow_model.load_state_dict(state_dict['net'])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(shadow_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# model training\n",
        "shadow_model.train()\n",
        "for epoch in range(80,n_epochs):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = shadow_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward() \n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() \n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "    # save the model after each 5 epochs\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        torch.save(shadow_model.state_dict(), f\"/content/drive/My Drive/Colab Notebooks/attack/my_models/tinyimagenet_mobilenetv2/shadow_model_epoch_{epoch+1}.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ggb_wppH6eg",
        "outputId": "ea9b2105-b4ce-4cbd-db52-49c62180b0df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load trained shadow model\n",
        "n_classes = 200\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "shadow_model = mobilenet_v2(num_classes=n_classes).to(device)\n",
        "shadow_model.load_state_dict(torch.load(\"/content/drive/My Drive/Colab Notebooks/attack/my_models/tinyimagenet_mobilenetv2/shadow_model_epoch_120.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "donDyB3NBXXc",
        "outputId": "9ad9c7a0-17a5-4173-d531-3c90c99a0641"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 4.6750744679930865, Accuracy: 30.18%\n"
          ]
        }
      ],
      "source": [
        "# model evaluation\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "shadow_model.eval()\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = shadow_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1) # get the class index\n",
        "        total += labels.size(0) # total samples\n",
        "        correct += (predicted == labels).sum().item() # correctly predicted samples\n",
        "\n",
        "print(f'Test Loss: {test_loss/len(test_loader)}, Accuracy: {100 * correct / total}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTPyZ7PgBaWR"
      },
      "source": [
        "# Attack Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4_57jxSjBboS"
      },
      "outputs": [],
      "source": [
        "# method for generating attack dataset for given model and dataset\n",
        "def generate_attack_data(model, data_loader, label):\n",
        "    attack_data = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            probabilities = nn.functional.softmax(outputs, dim=1)\n",
        "            for prob in probabilities:\n",
        "                attack_data.append((prob.cpu().numpy(), label))\n",
        "    return attack_data\n",
        "\n",
        "# generate attack data\n",
        "member_data = generate_attack_data(shadow_model, train_loader, 1) # member label as 1\n",
        "non_member_data = generate_attack_data(shadow_model, test_loader, 0) # non-member label as 0\n",
        "attack_data = member_data + non_member_data\n",
        "\n",
        "# prepare the data for training attack model\n",
        "np.random.shuffle(attack_data)\n",
        "X = np.array([x[0] for x in attack_data])\n",
        "y = np.array([x[1] for x in attack_data])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89ql2ETJK7Xb",
        "outputId": "68e79cdd-d4b3-4ffc-ff91-409b89cabd82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100, Loss: 0.2932\n",
            "Epoch 1/100, Test Loss: 0.0594\n",
            "Epoch 2/100, Loss: 0.2605\n",
            "Epoch 2/100, Test Loss: 0.0602\n",
            "Epoch 3/100, Loss: 0.2516\n",
            "Epoch 3/100, Test Loss: 0.0596\n",
            "Epoch 4/100, Loss: 0.2462\n",
            "Epoch 4/100, Test Loss: 0.0612\n",
            "Epoch 5/100, Loss: 0.2439\n",
            "Epoch 5/100, Test Loss: 0.0586\n",
            "Epoch 6/100, Loss: 0.2399\n",
            "Epoch 6/100, Test Loss: 0.0591\n",
            "Epoch 7/100, Loss: 0.2364\n",
            "Epoch 7/100, Test Loss: 0.0590\n",
            "Epoch 8/100, Loss: 0.2360\n",
            "Epoch 8/100, Test Loss: 0.0596\n",
            "Epoch 9/100, Loss: 0.2400\n",
            "Epoch 9/100, Test Loss: 0.0564\n",
            "Epoch 10/100, Loss: 0.2343\n",
            "Epoch 10/100, Test Loss: 0.0590\n",
            "Epoch 11/100, Loss: 0.2333\n",
            "Epoch 11/100, Test Loss: 0.0577\n",
            "Epoch 12/100, Loss: 0.2325\n",
            "Epoch 12/100, Test Loss: 0.0574\n",
            "Epoch 13/100, Loss: 0.2335\n",
            "Epoch 13/100, Test Loss: 0.0599\n",
            "Epoch 14/100, Loss: 0.2301\n",
            "Epoch 14/100, Test Loss: 0.0586\n",
            "Epoch 15/100, Loss: 0.2301\n",
            "Epoch 15/100, Test Loss: 0.0580\n",
            "Epoch 16/100, Loss: 0.2290\n",
            "Epoch 16/100, Test Loss: 0.0566\n",
            "Epoch 17/100, Loss: 0.2256\n",
            "Epoch 17/100, Test Loss: 0.0594\n",
            "Epoch 18/100, Loss: 0.2225\n",
            "Epoch 18/100, Test Loss: 0.0568\n",
            "Epoch 19/100, Loss: 0.2246\n",
            "Epoch 19/100, Test Loss: 0.0575\n",
            "Epoch 20/100, Loss: 0.2246\n",
            "Epoch 20/100, Test Loss: 0.0575\n",
            "Epoch 21/100, Loss: 0.2263\n",
            "Epoch 21/100, Test Loss: 0.0556\n",
            "Epoch 22/100, Loss: 0.2216\n",
            "Epoch 22/100, Test Loss: 0.0563\n",
            "Epoch 23/100, Loss: 0.2229\n",
            "Epoch 23/100, Test Loss: 0.0547\n",
            "Epoch 24/100, Loss: 0.2209\n",
            "Epoch 24/100, Test Loss: 0.0562\n",
            "Epoch 25/100, Loss: 0.2196\n",
            "Epoch 25/100, Test Loss: 0.0558\n",
            "Epoch 26/100, Loss: 0.2199\n",
            "Epoch 26/100, Test Loss: 0.0558\n",
            "Epoch 27/100, Loss: 0.2187\n",
            "Epoch 27/100, Test Loss: 0.0541\n",
            "Epoch 28/100, Loss: 0.2183\n",
            "Epoch 28/100, Test Loss: 0.0577\n",
            "Epoch 29/100, Loss: 0.2169\n",
            "Epoch 29/100, Test Loss: 0.0571\n",
            "Epoch 30/100, Loss: 0.2161\n",
            "Epoch 30/100, Test Loss: 0.0559\n",
            "Epoch 31/100, Loss: 0.2132\n",
            "Epoch 31/100, Test Loss: 0.0546\n",
            "Epoch 32/100, Loss: 0.2136\n",
            "Epoch 32/100, Test Loss: 0.0547\n",
            "Epoch 33/100, Loss: 0.2119\n",
            "Epoch 33/100, Test Loss: 0.0546\n",
            "Epoch 34/100, Loss: 0.2136\n",
            "Epoch 34/100, Test Loss: 0.0556\n",
            "Epoch 35/100, Loss: 0.2128\n",
            "Epoch 35/100, Test Loss: 0.0543\n",
            "Epoch 36/100, Loss: 0.2100\n",
            "Epoch 36/100, Test Loss: 0.0562\n",
            "Epoch 37/100, Loss: 0.2128\n",
            "Epoch 37/100, Test Loss: 0.0540\n",
            "Epoch 38/100, Loss: 0.2102\n",
            "Epoch 38/100, Test Loss: 0.0534\n",
            "Epoch 39/100, Loss: 0.2084\n",
            "Epoch 39/100, Test Loss: 0.0538\n",
            "Epoch 40/100, Loss: 0.2109\n",
            "Epoch 40/100, Test Loss: 0.0533\n",
            "Epoch 41/100, Loss: 0.2090\n",
            "Epoch 41/100, Test Loss: 0.0541\n",
            "Epoch 42/100, Loss: 0.2097\n",
            "Epoch 42/100, Test Loss: 0.0540\n",
            "Epoch 43/100, Loss: 0.2092\n",
            "Epoch 43/100, Test Loss: 0.0555\n",
            "Epoch 44/100, Loss: 0.2105\n",
            "Epoch 44/100, Test Loss: 0.0555\n",
            "Epoch 45/100, Loss: 0.2091\n",
            "Epoch 45/100, Test Loss: 0.0546\n",
            "Epoch 46/100, Loss: 0.2084\n",
            "Epoch 46/100, Test Loss: 0.0553\n",
            "Epoch 47/100, Loss: 0.2089\n",
            "Epoch 47/100, Test Loss: 0.0548\n",
            "Epoch 48/100, Loss: 0.2067\n",
            "Epoch 48/100, Test Loss: 0.0530\n",
            "Epoch 49/100, Loss: 0.2082\n",
            "Epoch 49/100, Test Loss: 0.0536\n",
            "Epoch 50/100, Loss: 0.2077\n",
            "Epoch 50/100, Test Loss: 0.0548\n",
            "Epoch 51/100, Loss: 0.2090\n",
            "Epoch 51/100, Test Loss: 0.0530\n",
            "Epoch 52/100, Loss: 0.2065\n",
            "Epoch 52/100, Test Loss: 0.0543\n",
            "Epoch 53/100, Loss: 0.2084\n",
            "Epoch 53/100, Test Loss: 0.0539\n",
            "Epoch 54/100, Loss: 0.2055\n",
            "Epoch 54/100, Test Loss: 0.0541\n",
            "Epoch 55/100, Loss: 0.2085\n",
            "Epoch 55/100, Test Loss: 0.0548\n",
            "Epoch 56/100, Loss: 0.2076\n",
            "Epoch 56/100, Test Loss: 0.0541\n",
            "Epoch 57/100, Loss: 0.2065\n",
            "Epoch 57/100, Test Loss: 0.0548\n",
            "Epoch 58/100, Loss: 0.2068\n",
            "Epoch 58/100, Test Loss: 0.0537\n",
            "Epoch 59/100, Loss: 0.2058\n",
            "Epoch 59/100, Test Loss: 0.0521\n",
            "Epoch 60/100, Loss: 0.2068\n",
            "Epoch 60/100, Test Loss: 0.0531\n",
            "Epoch 61/100, Loss: 0.2068\n",
            "Epoch 61/100, Test Loss: 0.0536\n",
            "Epoch 62/100, Loss: 0.2067\n",
            "Epoch 62/100, Test Loss: 0.0534\n",
            "Epoch 63/100, Loss: 0.2062\n",
            "Epoch 63/100, Test Loss: 0.0535\n",
            "Epoch 64/100, Loss: 0.2054\n",
            "Epoch 64/100, Test Loss: 0.0543\n",
            "Epoch 65/100, Loss: 0.2089\n",
            "Epoch 65/100, Test Loss: 0.0560\n",
            "Epoch 66/100, Loss: 0.2057\n",
            "Epoch 66/100, Test Loss: 0.0539\n",
            "Epoch 67/100, Loss: 0.2077\n",
            "Epoch 67/100, Test Loss: 0.0538\n",
            "Epoch 68/100, Loss: 0.2081\n",
            "Epoch 68/100, Test Loss: 0.0555\n",
            "Epoch 69/100, Loss: 0.2060\n",
            "Epoch 69/100, Test Loss: 0.0531\n",
            "Epoch 70/100, Loss: 0.2055\n",
            "Epoch 70/100, Test Loss: 0.0539\n",
            "Epoch 71/100, Loss: 0.2070\n",
            "Epoch 71/100, Test Loss: 0.0533\n",
            "Epoch 72/100, Loss: 0.2054\n",
            "Epoch 72/100, Test Loss: 0.0552\n",
            "Epoch 73/100, Loss: 0.2062\n",
            "Epoch 73/100, Test Loss: 0.0536\n",
            "Epoch 74/100, Loss: 0.2031\n",
            "Epoch 74/100, Test Loss: 0.0530\n",
            "Epoch 75/100, Loss: 0.2058\n",
            "Epoch 75/100, Test Loss: 0.0539\n",
            "Epoch 76/100, Loss: 0.2042\n",
            "Epoch 76/100, Test Loss: 0.0547\n",
            "Epoch 77/100, Loss: 0.2053\n",
            "Epoch 77/100, Test Loss: 0.0529\n",
            "Epoch 78/100, Loss: 0.2068\n",
            "Epoch 78/100, Test Loss: 0.0540\n",
            "Epoch 79/100, Loss: 0.2060\n",
            "Epoch 79/100, Test Loss: 0.0533\n",
            "Epoch 80/100, Loss: 0.2055\n",
            "Epoch 80/100, Test Loss: 0.0531\n",
            "Epoch 81/100, Loss: 0.2041\n",
            "Epoch 81/100, Test Loss: 0.0544\n",
            "Epoch 82/100, Loss: 0.2049\n",
            "Epoch 82/100, Test Loss: 0.0525\n",
            "Epoch 83/100, Loss: 0.2059\n",
            "Epoch 83/100, Test Loss: 0.0532\n",
            "Epoch 84/100, Loss: 0.2056\n",
            "Epoch 84/100, Test Loss: 0.0542\n",
            "Epoch 85/100, Loss: 0.2050\n",
            "Epoch 85/100, Test Loss: 0.0531\n",
            "Epoch 86/100, Loss: 0.1913\n",
            "Epoch 86/100, Test Loss: 0.0508\n",
            "Epoch 87/100, Loss: 0.1866\n",
            "Epoch 87/100, Test Loss: 0.0503\n",
            "Epoch 88/100, Loss: 0.1849\n",
            "Epoch 88/100, Test Loss: 0.0501\n",
            "Epoch 89/100, Loss: 0.1846\n",
            "Epoch 89/100, Test Loss: 0.0490\n",
            "Epoch 90/100, Loss: 0.1830\n",
            "Epoch 90/100, Test Loss: 0.0488\n",
            "Epoch 91/100, Loss: 0.1833\n",
            "Epoch 91/100, Test Loss: 0.0495\n",
            "Epoch 92/100, Loss: 0.1813\n",
            "Epoch 92/100, Test Loss: 0.0490\n",
            "Epoch 93/100, Loss: 0.1826\n",
            "Epoch 93/100, Test Loss: 0.0491\n",
            "Epoch 94/100, Loss: 0.1815\n",
            "Epoch 94/100, Test Loss: 0.0486\n",
            "Epoch 95/100, Loss: 0.1803\n",
            "Epoch 95/100, Test Loss: 0.0483\n",
            "Epoch 96/100, Loss: 0.1793\n",
            "Epoch 96/100, Test Loss: 0.0483\n",
            "Epoch 97/100, Loss: 0.1803\n",
            "Epoch 97/100, Test Loss: 0.0486\n",
            "Epoch 98/100, Loss: 0.1795\n",
            "Epoch 98/100, Test Loss: 0.0482\n",
            "Epoch 99/100, Loss: 0.1796\n",
            "Epoch 99/100, Test Loss: 0.0477\n",
            "Epoch 100/100, Loss: 0.1791\n",
            "Epoch 100/100, Test Loss: 0.0487\n",
            "Attack Model Accuracy: 0.942\n"
          ]
        }
      ],
      "source": [
        "# train attack model\n",
        "\n",
        "# attack model class\n",
        "class AttackModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AttackModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(200, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.bn4 = nn.BatchNorm1d(64)\n",
        "        self.fc5 = nn.Linear(64, 1)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(nn.functional.relu(self.bn1(self.fc1(x))))\n",
        "        x = self.dropout(nn.functional.relu(self.bn2(self.fc2(x))))\n",
        "        x = self.dropout(nn.functional.relu(self.bn3(self.fc3(x))))\n",
        "        x = self.dropout(nn.functional.relu(self.bn4(self.fc4(x))))\n",
        "        x = nn.functional.sigmoid(self.fc5(x))\n",
        "        return x\n",
        "\n",
        "# attack model instantiation\n",
        "attack_model_nn = AttackModel().to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(attack_model_nn.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
        "\n",
        "# prepare the data for the attack model\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
        "train_dataset_attack = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader_attack = DataLoader(train_dataset_attack, batch_size=64, shuffle=True)\n",
        "test_dataset_attack = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader_attack = DataLoader(test_dataset_attack, batch_size=64, shuffle=True)\n",
        "\n",
        "# training\n",
        "for epoch in range(100):\n",
        "    attack_model_nn.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader_attack:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = attack_model_nn(inputs)\n",
        "        loss = criterion(outputs, labels.view(-1, 1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    running_loss /= len(train_loader_attack)\n",
        "    scheduler.step(running_loss)\n",
        "    print(f\"Epoch {epoch+1}/100, Loss: {running_loss:.4f}\")\n",
        "\n",
        "    attack_model_nn.eval()\n",
        "    with torch.no_grad():\n",
        "      running_loss = 0.0\n",
        "      for inputs, labels in test_loader_attack:\n",
        "        outputs = attack_model_nn(inputs)\n",
        "        loss = criterion(outputs, labels.view(-1, 1))\n",
        "        running_loss += loss.item()\n",
        "\n",
        "      running_loss /= len(train_loader_attack)\n",
        "      print(f\"Epoch {epoch+1}/100, Test Loss: {running_loss:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "# evaluate the attack model on the test data\n",
        "attack_model_nn.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = attack_model_nn(X_test_tensor)\n",
        "    y_pred = outputs.round().view(-1).cpu().numpy()\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Attack Model Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwT9QeJ1Bl_O"
      },
      "source": [
        "# Target Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyd3qTg5Bm2j",
        "outputId": "36f402c0-b047-46fa-906b-a6e0751ea686"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MobileNetV2(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2dNormActivation(\n",
            "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU6(inplace=True)\n",
            "    )\n",
            "    (1): InvertedResidual(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (2): InvertedResidual(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
            "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (3): InvertedResidual(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
            "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (4): InvertedResidual(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
            "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (5): InvertedResidual(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
            "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (6): InvertedResidual(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
            "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (7): InvertedResidual(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
            "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (8): InvertedResidual(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
            "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (9): InvertedResidual(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
            "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (10): InvertedResidual(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
            "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (11): InvertedResidual(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
            "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (12): InvertedResidual(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
            "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (13): InvertedResidual(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
            "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (14): InvertedResidual(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
            "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (15): InvertedResidual(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
            "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (16): InvertedResidual(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
            "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (17): InvertedResidual(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
            "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU6(inplace=True)\n",
            "        )\n",
            "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (18): Conv2dNormActivation(\n",
            "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU6(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.2, inplace=False)\n",
            "    (1): Linear(in_features=1280, out_features=200, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# load the target model\n",
        "MODEL_PATH = '/content/drive/My Drive/Colab Notebooks/attack/models/mobilenetv2_tinyimagenet.pth'\n",
        "\n",
        "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "target_model = mobilenet_v2(num_classes=200).to(device)\n",
        "\n",
        "state_dict = torch.load(MODEL_PATH, map_location=device)\n",
        "target_model.load_state_dict(state_dict['net'])\n",
        "\n",
        "print(target_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IpdUYmbzBghK"
      },
      "outputs": [],
      "source": [
        "# loading evaluation data\n",
        "with open('/content/drive/My Drive/Colab Notebooks/attack/pickle/tinyimagenet/mobilenetv2/eval.p', 'rb') as f:\n",
        "    eval_dataset = pickle.load(f)\n",
        "\n",
        "# prepare the data for the evaluation\n",
        "eval_images = [item[0] for item in eval_dataset]\n",
        "eval_labels = [item[1] for item in eval_dataset]\n",
        "eval_membership_status = [item[2] for item in eval_dataset]\n",
        "\n",
        "eval_images_tensor = torch.stack(eval_images)\n",
        "eval_labels_tensor = torch.tensor(eval_labels)\n",
        "eval_dataset_tensor = TensorDataset(eval_images_tensor, eval_labels_tensor)\n",
        "eval_loader = DataLoader(eval_dataset_tensor, batch_size=64, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lIzeKMwq1S-P"
      },
      "outputs": [],
      "source": [
        "#  Generate evaluation data for attack model using target model\n",
        "target_model.eval()\n",
        "probabilities = []\n",
        "with torch.no_grad():\n",
        "    for inputs, _ in eval_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = target_model(inputs)\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        probabilities.extend(probs.cpu().numpy())\n",
        "\n",
        "X_eval = np.array(probabilities)\n",
        "y_eval_true = np.array(eval_membership_status)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZ_LHCU_BqUG",
        "outputId": "a8705009-9692-482c-a55c-1d46fb567ab3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Membership Inference Accuracy: 0.955\n"
          ]
        }
      ],
      "source": [
        "# evaluate the attack model\n",
        "attack_model_nn.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = attack_model_nn(torch.tensor(X_eval, dtype=torch.float32).to(device))\n",
        "    y_pred = outputs.round().view(-1).cpu().numpy()\n",
        "    accuracy = accuracy_score(y_eval_true, y_pred)\n",
        "    print(f\"Membership Inference Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kAraKd9_1-d4"
      },
      "outputs": [],
      "source": [
        "# save attack model\n",
        "torch.save(attack_model_nn.state_dict(), f\"/content/drive/My Drive/Colab Notebooks/attack/attack_models/mobilenetv2_tinyimagenet/attack_model_95.pt\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "nMoRjJvFBKUQ",
        "RTPyZ7PgBaWR",
        "ZwT9QeJ1Bl_O"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
